{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbLYe3967pc2"
      },
      "source": [
        "PREPARING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPiEIhebljgt",
        "outputId": "53f1fca7-5edf-423c-ff90-b53c4102e137"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "SOME_FIXED_SEED = 42\n",
        "\n",
        "# before training/inference:\n",
        "np.random.seed(SOME_FIXED_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gWKcKt1r4aq9"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "p_stemmer = PorterStemmer()\n",
        "topWordsLessMeaning = ['paper','research','study','ha','year',\n",
        "                       'group','example','wa','type','change','value',\n",
        "                       'work','source','resource','issue','show',\n",
        "                       'part','review','need','article','learning',\n",
        "                       'order','way','chapter','use','result','method',\n",
        "                       'approach','process','development','property',\n",
        "                       'model','parameter','simulation','science',\n",
        "                       'processing','project','technology','application',\n",
        "                       'analysis','problem']\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(topWordsLessMeaning)\n",
        "\n",
        "def proccessing_base(s):\n",
        "    \"\"\"\n",
        "    :param s: string to be processed\n",
        "    :return: processed string: see comments in the source code for more info\n",
        "    \"\"\"\n",
        "    # Loại từ đặc biệt\n",
        "    s = s.replace('\\xa0Read more', '')\n",
        "    # lower case\n",
        "    s = s.lower()\n",
        "    # Loại bỏ các dấu ngoặc\n",
        "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
        "    # normalization 10: ' ing ', noise text\n",
        "    s = re.sub(r' ing ', ' ', s)\n",
        "    # Loại bỏ dấu chấm\n",
        "    s = re.sub(r'[^\\w\\s]','',s)\n",
        "    # Loại bỏ dấu ,\n",
        "    s = s.replace(',', '')\n",
        "    # Loại bỏ số\n",
        "    s = re.sub(\"\\d+\", \"\", s)\n",
        "\n",
        "\n",
        "    return s.strip()\n",
        "\n",
        "def remove_word_nonenglish(text):\n",
        "  list_checked = [i if i.isalpha() else i[:-3] for i in text.split()]\n",
        "  return ' '.join(list_checked)\n",
        "\n",
        "def filter_noun(w_list):\n",
        "    return ' '.join([word for (word, pos) in nltk.pos_tag(w_list.split()) if pos[:2] == 'NN'])\n",
        "\n",
        "def stem(w_list):\n",
        "    return ' '.join([p_stemmer.stem(word) for word in w_list.split()])\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def lemmatizing(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(ele) for ele in text.split()])\n",
        "\n",
        "def findTheMostWordApperance(Doc, n_words = 40):\n",
        "    dicTF = {}\n",
        "    for d in Doc :\n",
        "      for word in d:\n",
        "        if dicTF.get(word) == None:\n",
        "          dicTF[word] = 1\n",
        "        else: dicTF[word] = dicTF[word] + 1\n",
        "    dictRe = {k: v for k, v in sorted(dicTF.items(), key=lambda item: item[1], reverse= True)}\n",
        "    count = 0\n",
        "    for item,value in dictRe.items():\n",
        "      if count > n_words :\n",
        "        break\n",
        "      else: print('sst : ',count,' từ : ',item, \"- số lượng : \", value)\n",
        "      count +=1\n",
        "def preprocessing(text):\n",
        "  corpus = []\n",
        "  for i in text:\n",
        "    i = proccessing_base(i)\n",
        "    i = lemmatizing(i)\n",
        "    i = filter_noun(i)\n",
        "    corpus.append(i)\n",
        "  corpus = remove_stopwords(corpus)\n",
        "  return corpus\n",
        "def removeTheMostWordLessMeaning(corpus):\n",
        "  result = []\n",
        "  for doc in corpus:\n",
        "     item = [value for value in doc if value not in topWordsLessMeaning]\n",
        "     result.append(item)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdziGvxOmSkT"
      },
      "source": [
        "LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "62rDneN1ZXp9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('train_id2word4.pkl', 'rb') as f:\n",
        "    train_id2word4 = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzPCnLJofAAQ"
      },
      "source": [
        "LOAD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nTXI6G_Je-uH"
      },
      "outputs": [],
      "source": [
        "## để load model thì phải tạo folder tên models bỏ hết file liên quan đến LDA vào\n",
        "import gensim\n",
        "lda_train = gensim.models.ldamulticore.LdaMulticore.load('models/lda_train2.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "y7LdaAhWSAYH"
      },
      "outputs": [],
      "source": [
        "def pipeline(docs,lda_train):\n",
        "  \"\"\"Trả về label cho của tác giả\"\"\"\n",
        "  # tien xu li\n",
        "  pre_docs = preprocessing(docs)\n",
        "\n",
        "  # bigram \n",
        "  bigram = gensim.models.Phrases(pre_docs, min_count = 15)\n",
        "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "  bigram = [bigram_mod[review] for review in pre_docs]\n",
        "  corpus = [train_id2word4.doc2bow(text) for text in bigram]\n",
        "\n",
        "  ## dự đoán cho mỗi đoạn văn\n",
        "  labels_topic = []\n",
        "  for topics in lda_train[corpus]:\n",
        "    topics_label_sort = sorted(topics[0], key=lambda x: x[1], reverse=True)\n",
        "    labels_topic.append(topics_label_sort[0][0])\n",
        "\n",
        "  ## Tìm chủ đề phổ biến\n",
        "  NumberOfTopic = [0]*9\n",
        "  for i in labels_topic :\n",
        "    NumberOfTopic[int(i)] += 1\n",
        "  return NumberOfTopic.index(max(NumberOfTopic))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxc0IGaVQXWU"
      },
      "source": [
        "Dự đoán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah7EKmTRQYZ5",
        "outputId": "86749a13-9981-42ce-e79e-ba9408c5d6fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tác giả thuộc nhãn :  3\n"
          ]
        }
      ],
      "source": [
        "### Load data các abstract của 1 tác giả\n",
        "data = [\n",
        "    \"Keyword extraction is an indispensable step formany natural language processing and information retrievalapplications such as; \\\n",
        "    text summarization and search engineoptimization. Keywords hold the most important informationdescribing the content of a document. \\\n",
        "    With the increasing volumeand variety of unlabeled documents on the Internet, the need forautomatic keyword extraction methods increases.\",\n",
        "    \" In this article we would like to present our experimental approach to automatic  keyphrases  extraction  based  on  statistical  methods\"\n",
        "]\n",
        "\n",
        "### dự đoán\n",
        "label = pipeline(data,lda_train)\n",
        "print(\"Author belong to : \",label)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b895285abb52695ec6ca1c961869f79eecea18e9005e064c900d9ad6c7b9b570"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
